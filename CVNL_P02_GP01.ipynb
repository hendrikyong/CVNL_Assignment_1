{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOwKA+BzGiUT5I8ttcgHxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hendrikyong/CVNL_Assignment_1/blob/main/CVNL_P02_GP01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. import dataset\n",
        "\n",
        "2. reorganise file for consistency (data preprocessing)\n",
        "data provided was\n",
        "Train set\n",
        "Folder A\n",
        "- A1.jpg\n",
        "- A2.jpg\n",
        "etc\n",
        "\n",
        "Test set\n",
        "- A_test.jpg\n",
        "- B_test.jpg\n",
        "etc\n",
        "\n",
        "reorganising it into\n",
        "Train set\n",
        "Folder A\n",
        "- A1.jpg\n",
        "- A2.jpg\n",
        "Folder B\n",
        "- B1.jpg\n",
        "- B2.jpg\n",
        "etc\n",
        "\n",
        "Test set\n",
        "Folder A\n",
        "- A_test.jpg\n",
        "Folder B\n",
        "- B_test.jpg\n",
        "etc\n",
        "\n",
        "3. define paths to the training and testing data directories\n",
        "'''"
      ],
      "metadata": {
        "id": "iOrn_W1vkfh1",
        "outputId": "71d55b85-bc35-4027-a929-697f11c08e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. import dataset\\n\\n2. reorganise file for consistency (data preprocessing)\\ndata provided was \\nTrain set \\nFolder A \\n- A1.jpg\\n- A2.jpg\\netc\\n\\nTest set\\n- A_test.jpg\\n- B_test.jpg\\netc\\n\\nreorganising it into \\nTrain set \\nFolder A \\n- A1.jpg\\n- A2.jpg\\nFolder B\\n- B1.jpg\\n- B2.jpg\\netc\\n\\nTest set\\nFolder A\\n- A_test.jpg\\nFolder B\\n- B_test.jpg\\netc\\n\\n3. define paths to the training and testing data directories\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dataset from kaggle\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "#download latest version\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "#list files in the dataset folder\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EzNs7EU3HlX",
        "outputId": "b6148fb5-4b27-4949-e7b4-7384f0f5e7f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1\n",
            "Files in the dataset: ['asl_alphabet_train', 'asl_alphabet_test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define paths to the training and testing data directories\n",
        "train_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_train/asl_alphabet_train'\n",
        "test_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_test/asl_alphabet_test'"
      ],
      "metadata": {
        "id": "gH-_CqItEriI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reorganise files\n",
        "#what is needed to be done is that from the test_dir for each image get first letter since all images are\n",
        "#labelled as letter_test.jpg and create a folder and add that image inside that folder\n",
        "import shutil\n",
        "\n",
        "for filename in os.listdir(test_dir):\n",
        "    if filename.endswith('.jpg'): #checks that it is a jpg\n",
        "        letter = filename.split('_')[0] # get the first part of the filename before '_'\n",
        "\n",
        "        # create a folder for the letter if it doesn't exist\n",
        "        letter_folder = os.path.join(test_dir, letter)\n",
        "        if not os.path.exists(letter_folder):\n",
        "            os.makedirs(letter_folder)\n",
        "        shutil.move(os.path.join(test_dir, filename), os.path.join(letter_folder, filename)) #moves the image into the corresponding folder\n",
        "\n",
        "print(\"Data re-organising complete\")"
      ],
      "metadata": {
        "id": "bxjSM7LnnUOx",
        "outputId": "b3c72609-91bf-453d-bf93-837bd7228c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data re-organising complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_bi8mnfd866Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model and train the CNN\n",
        "'''\n",
        "1. input data\n",
        "2. preprocessing\n",
        "  - standardize images(dont need to images from the dataset already comes 200x200)\n",
        "  - color transformation\n",
        "  - rotate image\n",
        "3. feature extraction\n",
        "4. ML model\n",
        "'''\n",
        "\n",
        "mean = [0.5,0.5,0.5]\n",
        "std = [0.5,0.5,0.5]\n",
        "\n",
        "#normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std) #need to change the mean and std but im not sure change to what though\n",
        "    #i think need to calc mean and std with like the train_loader len(dataset) thingy ?\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "16Ev39ZJ-LUu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model"
      ],
      "metadata": {
        "id": "t6hb_HKj9mmE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Load and organize data into a usable format.\n",
        "2. Resize and normalize the images to ensure consistency and optimal input for CNN.\n",
        "3. Apply data augmentation on the training data to avoid overfitting and enhance generalization.\n",
        "4. Split data into batches using DataLoader to handle larger datasets and speed up training.\n",
        "'''\n",
        "\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "#data augmentation transformation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  #randomly flip the image horizontally\n",
        "    transforms.RandomRotation(20),  #randomly rotate by a degree (-20 to 20)\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  #random color adjustments\n",
        "    transforms.RandomResizedCrop(224),  #randomly crop and resize to 224x224\n",
        "    transforms.ToTensor(),  #convert image to a tensor\n",
        "    transforms.Normalize(mean=mean, std=std),  #normalize with mean and std\n",
        "])\n",
        "\n",
        "#transformation to resize, normalize, and convert images to a tensor\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  #resize images to 224x224\n",
        "    transforms.ToTensor(),  #convert the image to a pytorch tensor\n",
        "    transforms.Normalize(mean=mean , std=std), #normalization\n",
        "])\n",
        "\n",
        "#load and transform dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "\n",
        "#create dataloader for batching the data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "gATnn1g5BByu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#possibly need to nn.sequential"
      ],
      "metadata": {
        "id": "SToQKQTD0bKG"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}