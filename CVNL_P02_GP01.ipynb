{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/aK0ARJWBdZ2F4RZWifUV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hendrikyong/CVNL_Assignment_1/blob/main/CVNL_P02_GP01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getting dataset\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "#download latest version\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "#list files in the dataset folder\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EzNs7EU3HlX",
        "outputId": "b6c03b5e-5d00-4c26-fc7b-9cea5f502218"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1\n",
            "Files in the dataset: ['asl_alphabet_train', 'asl_alphabet_test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_bi8mnfd866Z"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the paths to the training and testing data directories\n",
        "train_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_train/asl_alphabet_train'\n",
        "test_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_test/asl_alphabet_test'"
      ],
      "metadata": {
        "id": "gH-_CqItEriI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing\n",
        "'''\n",
        "1. Load and organize data into a usable format.\n",
        "- data is already into usable format\n",
        "2. Resize and normalize the images to ensure consistency and optimal input for CNN.\n",
        "3. Apply data augmentation on the training data to avoid overfitting and enhance generalization.\n",
        "4. Split data into batches using DataLoader to handle larger datasets and speed up training.\n",
        "'''\n",
        "\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "#data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  #randomly flip the image horizontally\n",
        "    transforms.RandomRotation(20),  #randomly rotate by a degree (-20 to 20)\n",
        "    transforms.RandomResizedCrop(224),  #randomly crop and resize the image to 224x224\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  #random color adjustments\n",
        "    transforms.Resize((224, 224)),  #resize to 224x224, if needed\n",
        "    transforms.ToTensor(),  #convert image to a tensor\n",
        "    transforms.Normalize(mean=mean, std=std),  #normalize with mean and std\n",
        "])\n",
        "\n",
        "#transformation to resize, normalize, and convert images to a tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  #resize images to 224x224\n",
        "    transforms.ToTensor(),  #convert the image to a pytorch tensor\n",
        "    transforms.Normalize(mean=mean , std=std), #normalization\n",
        "])\n",
        "\n",
        "#load and transform dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "#create DataLoader for batching the data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "#get one image and its label from the training dataset\n",
        "x_example, y_example = train_dataset[0]\n",
        "print(x_example.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gATnn1g5BByu",
        "outputId": "8919558c-71bf-43e3-ad9b-224dfd848d3c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    }
  ]
}