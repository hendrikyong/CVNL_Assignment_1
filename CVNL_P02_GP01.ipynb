{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQpXlFvdDPJRg2NlGDwvQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hendrikyong/CVNL_Assignment_1/blob/main/CVNL_P02_GP01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. import dataset\n",
        "\n",
        "2. reorganise file for consistency (data preprocessing)\n",
        "data provided was\n",
        "Train set\n",
        "Folder A\n",
        "- A1.jpg\n",
        "- A2.jpg\n",
        "etc\n",
        "\n",
        "Test set\n",
        "- A_test.jpg\n",
        "- B_test.jpg\n",
        "etc\n",
        "\n",
        "reorganising it into\n",
        "Train set\n",
        "Folder A\n",
        "- A1.jpg\n",
        "- A2.jpg\n",
        "Folder B\n",
        "- B1.jpg\n",
        "- B2.jpg\n",
        "etc\n",
        "\n",
        "Test set\n",
        "Folder A\n",
        "- A_test.jpg\n",
        "Folder B\n",
        "- B_test.jpg\n",
        "etc\n",
        "\n",
        "3. define paths to the training and testing data directories\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "iOrn_W1vkfh1",
        "outputId": "cdc6636b-fe10-449d-8dfb-0ed7d33a8db4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. import dataset\\n\\n2. reorganise file for consistency (data preprocessing)\\ndata provided was\\nTrain set\\nFolder A\\n- A1.jpg\\n- A2.jpg\\netc\\n\\nTest set\\n- A_test.jpg\\n- B_test.jpg\\netc\\n\\nreorganising it into\\nTrain set\\nFolder A\\n- A1.jpg\\n- A2.jpg\\nFolder B\\n- B1.jpg\\n- B2.jpg\\netc\\n\\nTest set\\nFolder A\\n- A_test.jpg\\nFolder B\\n- B_test.jpg\\netc\\n\\n3. define paths to the training and testing data directories\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dataset from kaggle\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "#download latest version\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "#list files in the dataset folder\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EzNs7EU3HlX",
        "outputId": "706c3437-dc47-46bd-c51d-f01e593d9810"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/grassknoted/asl-alphabet?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.03G/1.03G [00:12<00:00, 88.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1\n",
            "Files in the dataset: ['asl_alphabet_test', 'asl_alphabet_train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define paths to the training and testing data directories\n",
        "train_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_train/asl_alphabet_train'\n",
        "test_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_test/asl_alphabet_test'"
      ],
      "metadata": {
        "id": "gH-_CqItEriI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reorganise files\n",
        "#what is needed to be done is that from the test_dir for each image get first letter since all images are\n",
        "#labelled as letter_test.jpg and create a folder and add that image inside that folder\n",
        "import shutil\n",
        "\n",
        "for filename in os.listdir(test_dir):\n",
        "    if filename.endswith('.jpg'): #checks that it is a jpg\n",
        "        letter = filename.split('_')[0] # get the first part of the filename before '_'\n",
        "\n",
        "        # create a folder for the letter if it doesn't exist\n",
        "        letter_folder = os.path.join(test_dir, letter)\n",
        "        if not os.path.exists(letter_folder):\n",
        "            os.makedirs(letter_folder)\n",
        "        shutil.move(os.path.join(test_dir, filename), os.path.join(letter_folder, filename)) #moves the image into the corresponding folder\n",
        "\n",
        "print(\"Data re-organising complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxjSM7LnnUOx",
        "outputId": "e08e400f-9fb8-425f-e732-0304f39e2486"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data re-organising complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_bi8mnfd866Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model cnn model\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=29):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            #conv1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            #conv2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            #conv3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.25),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),  #flatten the tensor for fully connected layers\n",
        "            nn.Linear(128 * 8 * 8, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  #dropout for regularization\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "16Ev39ZJ-LUu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model and train the CNN\n",
        "'''\n",
        "1. input data\n",
        "2. preprocessing\n",
        "  - standardize images(dont need to images from the dataset already comes 200x200)\n",
        "  - color transformation\n",
        "  - rotate image\n",
        "3. feature extraction\n",
        "4. ML model\n",
        "'''\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "#normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std) #need to change the mean and std but im not sure change to what though\n",
        "    #i think need to calc mean and std with like the train_loader len(dataset) thingy ?\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "t6hb_HKj9mmE",
        "outputId": "a4a31962-00e6-4615-8b6c-0c87a306cf67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if gpu available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(device)\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "model = CNN(num_classes=29).cuda()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4) #changed to adam\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "zMUlw-pdUHZo",
        "outputId": "cae6c0f2-3e60-4ed6-f03a-b6c65d6e0eab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Dropout(p=0.25, inplace=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Dropout(p=0.25, inplace=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=8192, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=29, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, loss_func, optimizer, device, epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            #update tqdm with current metrics\n",
        "            progress_bar.set_postfix({\n",
        "                \"Loss\": f\"{epoch_loss / (batch_idx + 1):.4f}\",\n",
        "                \"Acc\": f\"{100. * correct / total:.2f}%\"\n",
        "            })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {100. * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "gATnn1g5BByu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, loss_func, device):\n",
        "    model.eval()  #set model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in progress_bar:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            #update tqdm with current metrics\n",
        "            progress_bar.set_postfix({\n",
        "                \"Loss\": f\"{test_loss / (total):.4f}\",\n",
        "                \"Acc\": f\"{100. * correct / total:.2f}%\"\n",
        "            })\n",
        "\n",
        "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Accuracy: {100. * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "SToQKQTD0bKG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, loss_func, optimizer, device, epochs)\n",
        "evaluate(model, test_loader, loss_func, device)"
      ],
      "metadata": {
        "id": "x4kzN3hY6P3z",
        "outputId": "93f68c40-9aaa-48de-83a6-89c9215e56b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 1360/1360 [02:19<00:00,  9.72it/s, Loss=1.4282, Acc=53.67%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss: 1.4282, Accuracy: 53.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 1360/1360 [02:23<00:00,  9.49it/s, Loss=0.5348, Acc=81.24%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss: 0.5348, Accuracy: 81.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 1360/1360 [02:21<00:00,  9.63it/s, Loss=0.3555, Acc=87.61%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss: 0.3555, Accuracy: 87.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 1360/1360 [02:20<00:00,  9.67it/s, Loss=0.2740, Acc=90.53%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss: 0.2740, Accuracy: 90.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 1360/1360 [02:23<00:00,  9.46it/s, Loss=0.2340, Acc=91.87%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss: 0.2340, Accuracy: 91.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 1360/1360 [02:22<00:00,  9.51it/s, Loss=0.2014, Acc=93.04%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Loss: 0.2014, Accuracy: 93.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 1360/1360 [02:21<00:00,  9.61it/s, Loss=0.1823, Acc=93.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Loss: 0.1823, Accuracy: 93.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 1360/1360 [02:22<00:00,  9.55it/s, Loss=0.1664, Acc=94.32%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Loss: 0.1664, Accuracy: 94.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 1360/1360 [02:20<00:00,  9.66it/s, Loss=0.1592, Acc=94.53%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Loss: 0.1592, Accuracy: 94.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 1360/1360 [02:23<00:00,  9.51it/s, Loss=0.1493, Acc=94.88%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Loss: 0.1493, Accuracy: 94.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 1360/1360 [02:20<00:00,  9.70it/s, Loss=0.1396, Acc=95.27%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Loss: 0.1396, Accuracy: 95.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 1360/1360 [02:23<00:00,  9.46it/s, Loss=0.1332, Acc=95.45%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Loss: 0.1332, Accuracy: 95.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 1360/1360 [02:21<00:00,  9.61it/s, Loss=0.1284, Acc=95.74%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Loss: 0.1284, Accuracy: 95.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 1360/1360 [02:22<00:00,  9.56it/s, Loss=0.1246, Acc=95.82%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Loss: 0.1246, Accuracy: 95.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 1360/1360 [02:23<00:00,  9.49it/s, Loss=0.1224, Acc=95.92%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Loss: 0.1224, Accuracy: 95.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 1360/1360 [02:19<00:00,  9.78it/s, Loss=0.1190, Acc=96.03%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Loss: 0.1190, Accuracy: 96.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 1360/1360 [02:23<00:00,  9.48it/s, Loss=0.1148, Acc=96.20%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Loss: 0.1148, Accuracy: 96.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 1360/1360 [02:21<00:00,  9.61it/s, Loss=0.1152, Acc=96.14%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Loss: 0.1152, Accuracy: 96.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 1360/1360 [02:22<00:00,  9.51it/s, Loss=0.1076, Acc=96.35%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Loss: 0.1076, Accuracy: 96.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 1360/1360 [02:20<00:00,  9.66it/s, Loss=0.1091, Acc=96.38%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Loss: 0.1091, Accuracy: 96.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s, Loss=0.0623, Acc=92.86%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.7432, Accuracy: 92.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}