{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSO3SbBUs+x8hppQBqn8rr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hendrikyong/CVNL_Assignment_1/blob/main/CVNL_P02_GP01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getting dataset\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "#download latest version\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "#list files in the dataset folder\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EzNs7EU3HlX",
        "outputId": "1ecb9056-1ec5-4aa1-f6be-550afaef6570"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1\n",
            "Files in the dataset: ['asl_alphabet_train', 'asl_alphabet_test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_bi8mnfd866Z"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the paths to the training and testing data directories\n",
        "train_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_train/asl_alphabet_train'\n",
        "test_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_test/asl_alphabet_test'"
      ],
      "metadata": {
        "id": "gH-_CqItEriI"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing\n",
        "'''\n",
        "1. Load and organize data into a usable format.\n",
        "2. Resize and normalize the images to ensure consistency and optimal input for CNN.\n",
        "3. Apply data augmentation on the training data to avoid overfitting and enhance generalization.\n",
        "4. Split data into batches using DataLoader to handle larger datasets and speed up training.\n",
        "'''\n",
        "\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "#data augmentation transformation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  #randomly flip the image horizontally\n",
        "    transforms.RandomRotation(20),  #randomly rotate by a degree (-20 to 20)\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  #random color adjustments\n",
        "    transforms.RandomResizedCrop(224),  #randomly crop and resize to 224x224\n",
        "    transforms.ToTensor(),  #convert image to a tensor\n",
        "    transforms.Normalize(mean=mean, std=std),  #normalize with mean and std\n",
        "])\n",
        "\n",
        "#transformation to resize, normalize, and convert images to a tensor\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  #resize images to 224x224\n",
        "    transforms.ToTensor(),  #convert the image to a pytorch tensor\n",
        "    transforms.Normalize(mean=mean , std=std), #normalization\n",
        "])\n",
        "\n",
        "#load and transform dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "\n",
        "#create dataloader for batching the data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "gATnn1g5BByu"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training"
      ],
      "metadata": {
        "id": "_FPRj6PnlbnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}