{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONsJVAgDE6f8Sa4QILvale",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hendrikyong/CVNL_Assignment_1/blob/main/CVNL_P02_GP01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dataset from kaggle\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "#download latest version\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "#list files in the dataset folder\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)\n",
        "\n",
        "#define paths to the training and testing data directories\n",
        "train_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_train/asl_alphabet_train'\n",
        "test_dir = '/root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_test/asl_alphabet_test'\n",
        "\n",
        "#reorganise files\n",
        "#what is needed to be done is that from the test_dir for each image get first letter since all images are\n",
        "#labelled as letter_test.jpg and create a folder and add that image inside that folder\n",
        "import shutil\n",
        "\n",
        "for filename in os.listdir(test_dir):\n",
        "    if filename.endswith('.jpg'): #checks that it is a jpg\n",
        "        letter = filename.split('_')[0] #get the first part of the filename before '_'\n",
        "\n",
        "        #create a folder for the letter if it doesn't exist\n",
        "        letter_folder = os.path.join(test_dir, letter)\n",
        "        if not os.path.exists(letter_folder):\n",
        "            os.makedirs(letter_folder)\n",
        "        shutil.move(os.path.join(test_dir, filename), os.path.join(letter_folder, filename)) #moves the image into the corresponding folder\n",
        "\n",
        "print(\"Data re-organising complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EzNs7EU3HlX",
        "outputId": "c69777e5-3efd-4f23-992f-d70303b3e38a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming download from 616562688 bytes (484324346 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/grassknoted/asl-alphabet?dataset_version_number=1 (616562688/1100887034) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.03G/1.03G [00:21<00:00, 22.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1\n",
            "Files in the dataset: ['asl_alphabet_test', 'asl_alphabet_train']\n",
            "Data re-organising complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_bi8mnfd866Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to calculate mean and std\n",
        "def calc_mean_std():\n",
        "  pass"
      ],
      "metadata": {
        "id": "2322pxsup3gG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model cnn model\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "'''\n",
        "typical architecture of a CNN\n",
        "1. input\n",
        "2. conv\n",
        "3. relu\n",
        "4. pooling\n",
        "5. fully connected layers\n",
        "5. output pred\n",
        "\n",
        "considerations:\n",
        "how many conv layers do i need for feature extraction?\n",
        "how many hidden layers?\n",
        "how many channels?\n",
        "'''\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=29):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            #conv1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            #conv2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            #conv3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),  #flatten the tensor for fully connected layers\n",
        "            nn.Linear(128 * 16 * 16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  #dropout for regularization\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "16Ev39ZJ-LUu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model and train the CNN\n",
        "'''\n",
        "1. input data\n",
        "2. preprocessing\n",
        "  - standardize images(dont need to images from the dataset already comes 200x200)\n",
        "  - color transformation\n",
        "  - rotate image\n",
        "3. feature extraction\n",
        "4. ML model\n",
        "'''\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "#normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomCrop(128, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std) #need to change the mean and std but im not sure change to what though\n",
        "    #i think need to calc mean and std with like the train_loader len(dataset) thingy ?\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "t6hb_HKj9mmE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if gpu available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(device)\n",
        "\n",
        "\n",
        "epochs = 7\n",
        "lr = 0.001\n",
        "model = CNN(num_classes=29)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "zMUlw-pdUHZo",
        "outputId": "5879b77e-2bcf-40ed-8394-28759d299a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (9): Dropout(p=0.25, inplace=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=32768, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=29, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, loss_func, optimizer, device, epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            #update tqdm with current metrics\n",
        "            progress_bar.set_postfix({\n",
        "                \"Loss\": f\"{epoch_loss / (batch_idx + 1):.4f}\",\n",
        "                \"Acc\": f\"{100. * correct / total:.2f}%\"\n",
        "            })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {100. * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "gATnn1g5BByu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, loss_func, device):\n",
        "    model.eval()  #set model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in progress_bar:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_func(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            #update tqdm with current metrics\n",
        "            progress_bar.set_postfix({\n",
        "                \"Loss\": f\"{test_loss / (total):.4f}\",\n",
        "                \"Acc\": f\"{100. * correct / total:.2f}%\"\n",
        "            })\n",
        "\n",
        "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Accuracy: {100. * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "SToQKQTD0bKG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, loss_func, optimizer, device, epochs)\n",
        "evaluate(model, test_loader, loss_func, device)"
      ],
      "metadata": {
        "id": "x4kzN3hY6P3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcca320a-1d9d-472a-b76e-79189caca916"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/7: 100%|██████████| 1360/1360 [05:13<00:00,  4.33it/s, Loss=2.1454, Acc=32.39%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss: 2.1454, Accuracy: 32.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/7: 100%|██████████| 1360/1360 [05:12<00:00,  4.36it/s, Loss=1.2326, Acc=56.98%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss: 1.2326, Accuracy: 56.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/7: 100%|██████████| 1360/1360 [05:10<00:00,  4.39it/s, Loss=0.9534, Acc=66.08%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss: 0.9534, Accuracy: 66.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/7: 100%|██████████| 1360/1360 [05:11<00:00,  4.37it/s, Loss=0.8065, Acc=71.33%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss: 0.8065, Accuracy: 71.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/7: 100%|██████████| 1360/1360 [05:09<00:00,  4.39it/s, Loss=0.7137, Acc=74.64%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss: 0.7137, Accuracy: 74.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/7: 100%|██████████| 1360/1360 [05:18<00:00,  4.27it/s, Loss=0.6532, Acc=76.73%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Loss: 0.6532, Accuracy: 76.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/7: 100%|██████████| 1360/1360 [05:26<00:00,  4.17it/s, Loss=0.6045, Acc=78.46%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Loss: 0.6045, Accuracy: 78.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s, Loss=0.0811, Acc=92.86%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.2701, Accuracy: 92.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}